import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def crawl_forms(url):
    """Fetches all <form> elements from a given page."""
    try:
        r = requests.get(url, timeout=8, allow_redirects=True)
    except requests.RequestException as e:
        print(f"[-] Crawler error @ {url}: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    forms = soup.find_all("form")
    print(f"[Crawler] Found {len(forms)} forms at {url}")
    return forms
